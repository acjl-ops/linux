From: "Ahmed S. Darwish" <a.darwish@linutronix.de>
Date: Wed, 10 Jun 2020 12:53:21 +0200
Subject: [PATCH 23/24] seqlock: PREEMPT_RT: Do not starve writers

Preemption must be disabled before entering a sequence counter write
side critical section. This cannot be done for PREEMPT_RT.

With disabled preemption:

  - The writer cannot be preempted by a task with higher priority

  - The writer cannot acquire a spinlock_t since it's a sleeping lock.
    This would invalidate the existing, and non-PREEMPT_RT valid, code
    pattern of acquiring a spinlock_t inside the seqcount write side
    critical section.

To remain preemptible, while avoiding a livelock caused by the reader
preempting the writer, use a different technique:

  - If the sequence counter is even upon entering a read side section,
    then no writer is in progress, and the reader did not preempt any
    write side sections. It can continue.

  - If the counter is odd, a writer is in progress and the reader may
    have preempted a write side section. Let the reader acquire the lock
    used for seqcount writer serialization, which is already held by the
    writer.

    The higher-priority reader will block on the lock, and the lower
    priority preempted writer will make progress until it finishes its
    write serialization lock critical section.

    Once the reader has the writer serialization lock acquired, the
    writer is finished and the counter is even. Drop the writer
    serialization lock and re-read the sequence counter.

Implement this technique for all PREEMPT_RT sleeping locks.

Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
---
 include/linux/seqlock.h                |   79 +++++++++++++++++--
 include/linux/seqlock_types_internal.h |  133 +++++++++++++++++++++++++++++++--
 2 files changed, 199 insertions(+), 13 deletions(-)

--- a/include/linux/seqlock.h
+++ b/include/linux/seqlock.h
@@ -479,6 +479,10 @@ static inline void write_seqcount_t_inva
  * See Documentation/locking/seqlock.rst
  */
 
+#if defined(CONFIG_LOCKDEP) || defined(CONFIG_PREEMPT_RT)
+#define SEQCOUNT_ASSOC_LOCK
+#endif
+
 /**
  * typedef seqcount_spinlock_t - sequence count with spinlock associated
  * @seqcount:		The real sequence counter
@@ -491,12 +495,12 @@ static inline void write_seqcount_t_inva
  */
 typedef struct seqcount_spinlock {
 	seqcount_t      seqcount;
-#ifdef CONFIG_LOCKDEP
+#ifdef SEQCOUNT_ASSOC_LOCK
 	spinlock_t	*lock;
 #endif
 } seqcount_spinlock_t;
 
-#ifdef CONFIG_LOCKDEP
+#ifdef SEQCOUNT_ASSOC_LOCK
 
 #define SEQCOUNT_LOCKTYPE_ZERO(seq_name, assoc_lock) {		\
 	.seqcount	= SEQCNT_ZERO(seq_name.seqcount),	\
@@ -510,7 +514,7 @@ do {								\
 	(s)->lock = (assoc_lock);				\
 } while (0)
 
-#else /* !CONFIG_LOCKDEP */
+#else /* !SEQCOUNT_ASSOC_LOCK */
 
 #define SEQCOUNT_LOCKTYPE_ZERO(seq_name, assoc_lock) {		\
 	.seqcount	= SEQCNT_ZERO(seq_name.seqcount),	\
@@ -521,7 +525,7 @@ do {								\
 	seqcount_init(&(s)->seqcount);				\
 } while (0)
 
-#endif
+#endif /* SEQCOUNT_ASSOC_LOCK */
 
 /**
  * SEQCNT_SPINLOCK_ZERO - static initializer for seqcount_spinlock_t
@@ -551,7 +555,7 @@ do {								\
  */
 typedef struct seqcount_raw_spinlock {
 	seqcount_t      seqcount;
-#ifdef CONFIG_LOCKDEP
+#ifdef SEQCOUNT_ASSOC_LOCK
 	raw_spinlock_t	*lock;
 #endif
 } seqcount_raw_spinlock_t;
@@ -584,7 +588,7 @@ typedef struct seqcount_raw_spinlock {
  */
 typedef struct seqcount_rwlock {
 	seqcount_t      seqcount;
-#ifdef CONFIG_LOCKDEP
+#ifdef SEQCOUNT_ASSOC_LOCK
 	rwlock_t	*lock;
 #endif
 } seqcount_rwlock_t;
@@ -620,7 +624,7 @@ typedef struct seqcount_rwlock {
  */
 typedef struct seqcount_mutex {
 	seqcount_t      seqcount;
-#ifdef CONFIG_LOCKDEP
+#ifdef SEQCOUNT_ASSOC_LOCK
 	struct mutex	*lock;
 #endif
 } seqcount_mutex_t;
@@ -656,7 +660,7 @@ typedef struct seqcount_mutex {
  */
 typedef struct seqcount_ww_mutex {
 	seqcount_t      seqcount;
-#ifdef CONFIG_LOCKDEP
+#ifdef SEQCOUNT_ASSOC_LOCK
 	struct ww_mutex	*lock;
 #endif
 } seqcount_ww_mutex_t;
@@ -727,11 +731,70 @@ typedef struct {
  *
  * Return: count to be passed to read_seqretry()
  */
+
+/*
+ * For PREEMPT_RT, preemption cannot be disabled upon entering the write
+ * side critical section. With disabled preemption:
+ *
+ *   - The writer cannot be preempted by a task with higher priority
+ *
+ *   - The writer cannot acquire a spinlock_t since it's a sleeping
+ *     lock.  This would invalidate the existing, and non-PREEMPT_RT
+ *     valid, code pattern of acquiring a spinlock_t inside the seqcount
+ *     write side critical section.
+ *
+ * To remain preemptible, while avoiding a livelock caused by the reader
+ * preempting the writer, use a different technique:
+ *
+ *   - If the sequence counter is even upon entering a read side
+ *     section, then no writer is in progress, and the reader did not
+ *     preempt any write side sections. It can continue.
+ *
+ *   - If the counter is odd, a writer is in progress and the reader may
+ *     have preempted a write side section. Let the reader acquire the
+ *     lock used for seqcount writer serialization, which is already
+ *     held by the writer.
+ *
+ *     The higher-priority reader will block on the lock, and the
+ *     lower-priority preempted writer will make progress until it
+ *     finishes its write serialization lock critical section.
+ *
+ *     Once the reader has the writer serialization lock acquired, the
+ *     writer is finished and the counter is even. Drop the writer
+ *     serialization lock and re-read the sequence counter.
+ *
+ * This technique must be implemented for all PREEMPT_RT sleeping locks.
+ */
+#ifdef CONFIG_PREEMPT_RT
+
+static inline unsigned read_seqbegin(const seqlock_t *sl)
+{
+	unsigned seq;
+
+	seqcount_lockdep_reader_access(&sl->seqcount);
+
+	do {
+		seq = READ_ONCE(sl->seqcount.sequence);
+		if (unlikely(seq & 1)) {
+			seqlock_t *msl = (seqlock_t *)sl;
+			spin_lock(&msl->lock);
+			spin_unlock(&msl->lock);
+		}
+	} while (unlikely(seq & 1));
+
+	smp_rmb();
+	return seq;
+}
+
+#else /* !CONFIG_PREEMPT_RT */
+
 static inline unsigned read_seqbegin(const seqlock_t *sl)
 {
 	return read_seqcount_t_begin(&sl->seqcount);
 }
 
+#endif
+
 /**
  * read_seqretry() - end and validate a seqlock_t read side section
  * @sl: Pointer to &typedef seqlock_t
--- a/include/linux/seqlock_types_internal.h
+++ b/include/linux/seqlock_types_internal.h
@@ -51,6 +51,16 @@
  * Never use lockdep for the raw write variants.
  */
 
+#ifdef CONFIG_PREEMPT_RT
+
+/*
+ * Do not disable preemption for PREEMPT_RT. Check comment on top of
+ * seqlock.h read_seqbegin() for rationale.
+ */
+#define __enforce_preemption_protection(s)			(false)
+
+#else
+
 #define __associated_lock_is_preemptible(s)				\
 ({									\
 	bool ret;							\
@@ -69,6 +79,11 @@
 	ret;								\
 })
 
+#define __enforce_preemption_protection(s)				\
+	__associated_lock_is_preemptible(s)
+
+#endif /* CONFIG_PREEMPT_RT */
+
 #ifdef CONFIG_LOCKDEP
 
 #define __assert_associated_lock_held(s)				\
@@ -101,7 +116,7 @@ do {									\
 
 #define do_raw_write_seqcount_begin(s)					\
 do {									\
-	if (__associated_lock_is_preemptible(s))			\
+	if (__enforce_preemption_protection(s))				\
 		preempt_disable();					\
 									\
 	raw_write_seqcount_t_begin(__to_seqcount_t(s));			\
@@ -111,7 +126,7 @@ do {									\
 do {									\
 	raw_write_seqcount_t_end(__to_seqcount_t(s));			\
 									\
-	if (__associated_lock_is_preemptible(s))			\
+	if (__enforce_preemption_protection(s))				\
 		preempt_enable();					\
 } while (0)
 
@@ -119,7 +134,7 @@ do {									\
 do {									\
 	__assert_associated_lock_held(s);				\
 									\
-	if (__associated_lock_is_preemptible(s))			\
+	if (__enforce_preemption_protection(s))				\
 		preempt_disable();					\
 									\
 	write_seqcount_t_begin_nested(__to_seqcount_t(s), subclass);	\
@@ -129,7 +144,7 @@ do {									\
 do {									\
 	__assert_associated_lock_held(s);				\
 									\
-	if (__associated_lock_is_preemptible(s))			\
+	if (__enforce_preemption_protection(s))				\
 		preempt_disable();					\
 									\
 	write_seqcount_t_begin(__to_seqcount_t(s));			\
@@ -139,7 +154,7 @@ do {									\
 do {									\
 	write_seqcount_t_end(__to_seqcount_t(s));			\
 									\
-	if (__associated_lock_is_preemptible(s))			\
+	if (__enforce_preemption_protection(s))				\
 		preempt_enable();					\
 } while (0)
 
@@ -160,6 +175,108 @@ do {									\
  *	seqcount_LOCKTYPE_t -- read APIs
  */
 
+#ifdef CONFIG_PREEMPT_RT
+
+/*
+ * Check comment on top of read_seqbegin() for rationale.
+ *
+ * @s: pointer to seqcount_t or any of the seqcount_locktype_t variants
+ */
+#define __rt_lock_unlock_associated_sleeping_lock(s)			\
+do {									\
+	if (__same_type(*(s), seqcount_t)  ||				\
+	    __same_type(*(s), seqcount_raw_spinlock_t))	{		\
+		break;	/* NOP */					\
+	}								\
+									\
+	if (__same_type(*(s), seqcount_spinlock_t)) {			\
+		spin_lock(((seqcount_spinlock_t *) s)->lock);		\
+		spin_unlock(((seqcount_spinlock_t *) s)->lock);		\
+	} else if (__same_type(*(s), seqcount_rwlock_t)) {		\
+		read_lock(((seqcount_rwlock_t *) s)->lock);		\
+		read_unlock(((seqcount_rwlock_t *) s)->lock);		\
+	} else if (__same_type(*(s), seqcount_mutex_t)) {		\
+		mutex_lock(((seqcount_mutex_t *) s)->lock);		\
+		mutex_unlock(((seqcount_mutex_t *) s)->lock);		\
+	} else if (__same_type(*(s), seqcount_ww_mutex_t)) {		\
+		ww_mutex_lock(((seqcount_ww_mutex_t *) s)->lock, NULL); \
+		ww_mutex_unlock(((seqcount_ww_mutex_t *) s)->lock);	\
+	} else								\
+		BUILD_BUG_ON_MSG(1, "Unknown seqcount type");		\
+} while (0)
+
+/*
+ * @s: pointer to seqcount_t or any of the seqcount_locktype_t variants
+ *
+ * After the lock-unlock operation, re-read the sequence counter since
+ * the writer made progress.
+ *
+ * Do not lock-unlock the seqcount associated sleeping lock again if the
+ * second counter read value is odd. If the first counter read was odd
+ * because the reader preempted the write-side critical section, the
+ * second odd value read must've been the result of a writer running on
+ * a parallel core instead.
+ */
+#define __raw_read_seqcount(s)						\
+({									\
+	unsigned seq = READ_ONCE(__to_seqcount_t(s)->sequence);		\
+									\
+	if (unlikely(seq & 1))						\
+		__rt_lock_unlock_associated_sleeping_lock(s);		\
+									\
+	/* no read barrier, no counter stabilization, no lockdep */	\
+	READ_ONCE(__to_seqcount_t(s)->sequence);			\
+})
+
+#define do___read_seqcount_begin(s)					\
+({									\
+	unsigned seq;							\
+									\
+	do {								\
+		seq = __raw_read_seqcount(s);				\
+		cpu_relax();						\
+	} while (unlikely(seq & 1));					\
+									\
+	/* no read barrier, with stabilized counter, no lockdep */	\
+	seq;								\
+})
+
+#define do_raw_read_seqcount(s)						\
+({									\
+	unsigned seq = __raw_read_seqcount(s);				\
+									\
+	smp_rmb();							\
+									\
+	/* with read barrier, no counter stabilization, no lockdep */	\
+	seq;								\
+})
+
+#define do_raw_seqcount_begin(s)					\
+({									\
+	/* with read barrier, no counter stabilization, no lockdep */	\
+	(do_raw_read_seqcount(s) & ~1);					\
+})
+
+#define do_raw_read_seqcount_begin(s)					\
+({									\
+	unsigned seq = do___read_seqcount_begin(s);			\
+									\
+	smp_rmb();							\
+									\
+	/* with read barrier, with stabilized counter, no lockdep */	\
+	seq;								\
+})
+
+#define do_read_seqcount_begin(s)					\
+({									\
+	seqcount_lockdep_reader_access(__to_seqcount_t(s));		\
+									\
+	/* with read barrier, stabilized counter, and lockdep */	\
+	do_raw_read_seqcount_begin(s);					\
+})
+
+#else /* !CONFIG_PREEMPT_RT */
+
 #define do___read_seqcount_begin(s)					\
 	__read_seqcount_t_begin(__to_seqcount_t(s))
 
@@ -175,6 +292,12 @@ do {									\
 #define do_read_seqcount_begin(s)					\
 	read_seqcount_t_begin(__to_seqcount_t(s))
 
+#endif /* CONFIG_PREEMPT_RT */
+
+/*
+ * Latch sequence counters allows interruptible, preemptible, writer
+ * sections. There is no need for a special PREEMPT_RT implementation.
+ */
 #define do_raw_read_seqcount_latch(s)					\
 	raw_read_seqcount_t_latch(__to_seqcount_t(s))
 
